# ğŸ§  NeuroSense: Multimodal Sentiment & Emotion Classifier

![neurosense-fusion](https://socialify.git.ci/lalitdotdev/neurosense-fusion/image?description=1&font=Inter&language=1&logo=https%3A%2F%2Fstellarix.com%2Fwp-content%2Fuploads%2F2024%2F07%2FMultimodal-AI.jpg&name=1&owner=1&pattern=Solid&stargazers=1&theme=Light)

**Decode human emotion and sentiment from video, audio, and textâ€”at scale, in real-time, and with research-grade accuracy.**

---

## ğŸ“– Overview

**NeuroSense** is a next-generation multimodal AI framework that fuses video, audio, and text to recognize emotions and sentiments in human communication. Designed for research, real-world deployment, and SaaS applications, NeuroSense combines the power of deep learning, cloud scalability, and a modern web interface.

---

## ğŸš€ Features

- ğŸ¥ **Video Frame Analysis** â€” Extracts facial and contextual cues using ResNet3D.
- ğŸ™ï¸ **Audio Feature Extraction** â€” Captures vocal emotion with Mel spectrograms and CNNs.
- ğŸ“ **Text Embeddings with BERT** â€” Understands semantic sentiment from transcripts.
- ğŸ”— **Multimodal Fusion** â€” Late fusion of 128D features from each modality for robust affect detection.
- ğŸ“Š **Dual Head Classification** â€” Simultaneous prediction of 7 emotion classes and 3 sentiment classes.
- ğŸ§ª **Model Training & Evaluation** â€” Efficient PyTorch pipeline with TensorBoard logging.
- â˜ï¸ **Scalable Cloud Deployment** â€” AWS SageMaker for training, S3 for data, and real-time inference endpoints.
- ğŸ” **Authentication & API Keys** â€” Auth.js and secure key management for SaaS users.
- ğŸ“ˆ **Usage Quota Tracking** â€” Monitor and limit API usage per user.
- ğŸŒ **Modern Frontend** â€” Next.js, Tailwind CSS, and T3 Stack for a seamless user experience.
- ğŸ–¼ï¸ **Rich Visualizations** â€” Confusion matrices, training curves, and interactive analytics.

---

## ğŸ—ï¸ Model Architecture

```
Video Frames â”€â”
              â”‚
         [ResNet3D]â”€â”€â”
Text â”€â”€â”€â”€â”€[BERT]â”€â”€â”€â”€â”€â”¼â”€â–º [Fusion Layer] â”€â”€â–º [Emotion Classifier] â”€â–º 7 Emotions
              â”‚      â”‚                    â””â”€â–º [Sentiment Classifier] â”€â–º 3 Sentiments
Audio â”€â”€[CNN+Mel]â”€â”€â”€â”€â”˜
```

![Model Architecture](assets/architecture.png)

### Key Components

- **Input Modalities**: Video frames, audio clips, and text transcripts
- **Feature Extraction**:
  - Video: ResNet3D processes frames to extract spatial-temporal features.
  - Audio: CNN processes Mel spectrograms for vocal emotion.
  - Text: BERT generates contextual embeddings from transcripts.
- **Fusion Layer**: Concatenates features from all modalities into a unified representation.
- **Classification Heads**:
  - Emotion Classifier: 7-way softmax for emotions (e.g., happy, sad, angry).
  - Sentiment Classifier: 3-way softmax for sentiment (positive, negative, neutral).
- **Output**: Real-time predictions for both emotion and sentiment.

### Model Details

- **Encoders**: BERT (text), ResNet3D (video), CNN (audio)
- **Fusion**: Concatenates 128D features from each encoder (total 384D), then projects to 256D
- **Heads**: Two classifiers for emotion (7-way) and sentiment (3-way)

---

## ğŸ› ï¸ Tech Stack

| Layer         | Technologies                                                                 |
| ------------- | ---------------------------------------------------------------------------- |
| **AI/ML**     | PyTorch, HuggingFace Transformers (BERT), TorchVision (ResNet3D), torchaudio |
| **Cloud**     | AWS SageMaker, S3, IAM, CloudWatch, Docker                                   |
| **Web**       | Next.js, React, Tailwind CSS, tRPC, Prisma, Auth.js                          |
| **Dev Tools** | TensorBoard, Matplotlib, Seaborn, Docker                                     |

---

## ğŸ“¦ Installation & Setup

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/neurosense.git
cd neurosense
```

### 2. Install Python Dependencies

```bash
pip install -r requirements.txt
```

### 3. Install Frontend Dependencies

```bash
cd frontend
npm install
cd ..
```

### 4. Prepare Dataset

- Download the [MELD Dataset](https://affective-meld.github.io)
- Extract and place it in the `data/` directory as follows:
  ```
  data/
    â”œâ”€â”€ train_splits/
    â”œâ”€â”€ test_splits/
    â””â”€â”€ dev_splits/
  ```

---

## ğŸ§  Model Training (Local or SageMaker)

### Local Training

```bash
python train.py --model-dir ./output --epochs 25 --data-dir ./data
```

### AWS SageMaker Training

1. **Increase Quota** for your desired instance type (e.g., `ml.g5.xlarge`).
2. **Upload Dataset** to your S3 bucket:
   ```bash
   aws s3 sync ./data s3://your-bucket/data
   ```
3. **Create IAM Role** with S3 and SageMaker permissions.
4. **Start Training Job**:
   ```bash
   python train_sagemaker.py --role-arn
   ```

---

## ğŸ›°ï¸ Model Deployment

1. **Upload Model Artifacts** to your S3 bucket after training.
2. **Deploy Endpoint**:
   ```bash
   python deploy_endpoint.py --model-s3-uri s3://your-bucket/model.tar.gz
   ```
3. **Configure IAM for Inference** (see `deployment/README.md` for details).

---

## ğŸ“ Inference API

- **REST API**: Real-time predictions via SageMaker endpoint.
- **API Key Management**: Secure access for frontend and external clients.
- **Example Usage**:
  ```python
  import requests
  response = requests.post(
      "https://api.neurosense.app/infer",
      headers={"x-api-key": ""},
      files={"video": open("sample.mp4", "rb")}
  )
  print(response.json())
  ```

---

## ğŸ“Š TensorBoard & Visualization

- **Training Metrics**:
  ```bash
  tensorboard --logdir output/tensorboard
  ```
- **Confusion Matrices & Curves**:
  Check `output/` or `results/` for PNGs and CSVs.

---

## ğŸ–¥ï¸ Frontend Web App

**NeuroSense** includes a modern SaaS dashboard built with **Next.js** and **Tailwind CSS**.

### **Frontend Features**

- ğŸ¬ **Media Upload**: Drag-and-drop video/audio files
- ğŸ“ **Text Input**: Paste or type transcript for analysis
- âš¡ **Real-Time Inference**: See emotion & sentiment predictions instantly
- ğŸ“ˆ **Interactive Visualizations**: Explore confusion matrices, training curves, and usage analytics
- ğŸ”‘ **Authentication**: Secure sign-in with Auth.js (Google, GitHub, etc.)
- ğŸ“Š **Usage Dashboard**: Track API calls and quota per user
- ğŸ›¡ï¸ **API Key Management**: Generate and manage API keys for secure access

### **Run the Frontend Locally**

```bash
cd frontend
npm run dev
# Visit http://localhost:3000
```

---

## ğŸ“Š Example Results

| Modality             | Emotion Accuracy | Sentiment F1 |
| -------------------- | ---------------- | ------------ |
| Video + Audio + Text | 0.82             | 0.87         |

- **Confusion matrices** and **classification reports** are auto-generated and saved for every evaluation.

---

## ğŸŒ Applications

- **Conversational AI**: Enhance chatbots and virtual assistants with emotional intelligence
- **Customer Experience**: Analyze emotions in support calls and video chats
- **Content Moderation**: Detect toxic or harmful sentiments in user-generated content
- **Mental Health**: Monitor mood and affect in telehealth sessions
- **Education**: Track student engagement and sentiment in e-learning

---

## ğŸ’¡ Inspiration

NeuroSense is built to demonstrate the full lifecycle of a multimodal AI applicationâ€”from deep learning model training, to scalable cloud deployment, to a beautiful SaaS web interface. Itâ€™s ideal for researchers, engineers, and product teams exploring the future of affective computing.

---

## ğŸ“¬ Contributions

Pull requests are welcome! For major changes, please open an issue first to discuss what you would like to change.

---

## ğŸ“„ License

[MIT](LICENSE)

---

## â­ï¸ Give NeuroSense a Star!

If you find this project useful or inspiring, please consider starring the repo and sharing it with your network!

---

**Decode the unspoken. Understand the unseen. Welcome to the future of emotion-aware AI.**

---
